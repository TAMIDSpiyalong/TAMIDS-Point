{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc75236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "from skimage import io, transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f130644-e927-4775-9f8d-864581f853de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d9ce5",
   "metadata": {},
   "source": [
    "# Make folder to save model and tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53380232",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_folder='20230824_2/'\n",
    "\n",
    "output_dir=os.path.join(date_folder,'out/')\n",
    "\n",
    "tensorboard_dir=os.path.join(date_folder,'tensorboard/')\n",
    "checkpoints_dir=os.path.join(date_folder,'ckpts/')\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "if not os.path.exists(tensorboard_dir):\n",
    "    os.makedirs(tensorboard_dir)\n",
    "if not os.path.exists(checkpoints_dir):\n",
    "    os.makedirs(checkpoints_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c8a2077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20230824_2/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cde6ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20230824_2/tensorboard/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181b08fb",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8e051c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Backbone_VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Backbone_VGG, self).__init__()\n",
    "        self.body1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.body2 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.body3 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.body4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.body1(x)\n",
    "        out2 = self.body2(out1)\n",
    "        out3 = self.body3(out2)\n",
    "        out4 = self.body4(out3)\n",
    "        \n",
    "        return out1, out2, out3, out4\n",
    "\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.act4 = nn.ReLU()\n",
    "        self.output = nn.Conv2d(256, 8, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.act1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.act2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.act3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.act4(out)\n",
    "        out = self.output(out)\n",
    "        \n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "\n",
    "        return out.contiguous().view(out.shape[0], -1, 2)\n",
    "\n",
    "\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, num_features_in=256, num_anchor_points=4, num_classes=2, prior=0.01, feature_size=256):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchor_points = num_anchor_points\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.Conv2d(feature_size, num_anchor_points * num_classes, kernel_size=3, padding=1)\n",
    "        self.output_act = nn.Sigmoid()\n",
    "        \n",
    "    # sub-branch forward\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.act1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.act2(out)\n",
    "\n",
    "        out = self.output(out)\n",
    "\n",
    "        out1 = out.permute(0, 2, 3, 1)\n",
    "\n",
    "        batch_size, width, height, _ = out1.shape\n",
    "\n",
    "        out2 = out1.view(batch_size, width, height, self.num_anchor_points, self.num_classes)\n",
    "\n",
    "        return out2.contiguous().view(x.shape[0], -1, self.num_classes)\n",
    "\n",
    "class AnchorPoints(nn.Module):\n",
    "    def __init__(self, pyramid_levels=None, strides=None, row=3, line=3):\n",
    "        super(AnchorPoints, self).__init__()\n",
    "\n",
    "        if pyramid_levels is None:\n",
    "            self.pyramid_levels = [3, 4, 5, 6, 7]\n",
    "        else:\n",
    "            self.pyramid_levels = pyramid_levels\n",
    "\n",
    "        if strides is None:\n",
    "            self.strides = [2 ** x for x in self.pyramid_levels]\n",
    "\n",
    "        self.row = row\n",
    "        self.line = line\n",
    "\n",
    "    def forward(self, image):\n",
    "        image_shape = image.shape[2:]\n",
    "        image_shape = np.array(image_shape)\n",
    "        image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels]\n",
    "\n",
    "        all_anchor_points = np.zeros((0, 2)).astype(np.float32)\n",
    "        # get reference points for each level\n",
    "        for idx, p in enumerate(self.pyramid_levels):\n",
    "            anchor_points = generate_anchor_points(2**p, row=self.row, line=self.line)\n",
    "            shifted_anchor_points = shift(image_shapes[idx], self.strides[idx], anchor_points)\n",
    "            all_anchor_points = np.append(all_anchor_points, shifted_anchor_points, axis=0)\n",
    "\n",
    "        all_anchor_points = np.expand_dims(all_anchor_points, axis=0)\n",
    "        # send reference points to device\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.from_numpy(all_anchor_points.astype(np.float32)).cuda()\n",
    "        else:\n",
    "            return torch.from_numpy(all_anchor_points.astype(np.float32))\n",
    "def generate_anchor_points(stride=16, row=3, line=3):\n",
    "    row_step = stride / row\n",
    "    line_step = stride / line\n",
    "\n",
    "    shift_x = (np.arange(1, line + 1) - 0.5) * line_step - stride / 2\n",
    "    shift_y = (np.arange(1, row + 1) - 0.5) * row_step - stride / 2\n",
    "\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "\n",
    "    anchor_points = np.vstack((\n",
    "        shift_x.ravel(), shift_y.ravel()\n",
    "    )).transpose()\n",
    "\n",
    "    return anchor_points\n",
    "\n",
    "def shift(shape, stride, anchor_points):\n",
    "    shift_x = (np.arange(0, shape[1]) + 0.5) * stride\n",
    "    shift_y = (np.arange(0, shape[0]) + 0.5) * stride\n",
    "\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "\n",
    "    shifts = np.vstack((\n",
    "        shift_x.ravel(), shift_y.ravel()\n",
    "    )).transpose()\n",
    "\n",
    "    A = anchor_points.shape[0]\n",
    "    K = shifts.shape[0]\n",
    "    all_anchor_points = (anchor_points.reshape((1, A, 2)) + shifts.reshape((1, K, 2)).transpose((1, 0, 2)))\n",
    "    all_anchor_points = all_anchor_points.reshape((K * A, 2))\n",
    "\n",
    "    return all_anchor_points\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, C3_size, C4_size, C5_size, feature_size=256):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # upsample C5 to get P5 from the FPN paper\n",
    "        self.P5_1 = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P5_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.P5_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # add P5 elementwise to C4\n",
    "        self.P4_1 = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P4_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.P4_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # add P4 elementwise to C3\n",
    "        self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P3_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        C3, C4, C5 = inputs\n",
    "\n",
    "        P5_x = self.P5_1(C5)\n",
    "        P5_upsampled_x = self.P5_upsampled(P5_x)\n",
    "        P5_x = self.P5_2(P5_x)\n",
    "\n",
    "        P4_x = self.P4_1(C4)\n",
    "        P4_x = P5_upsampled_x + P4_x\n",
    "        P4_upsampled_x = self.P4_upsampled(P4_x)\n",
    "        P4_x = self.P4_2(P4_x)\n",
    "\n",
    "        P3_x = self.P3_1(C3)\n",
    "        P3_x = P3_x + P4_upsampled_x\n",
    "        P3_x = self.P3_2(P3_x)\n",
    "\n",
    "        return [P3_x, P4_x, P5_x]\n",
    "\n",
    "\n",
    "class P2PNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(P2PNet, self).__init__()\n",
    "        self.backbone = Backbone_VGG()\n",
    "        self.regression = RegressionModel()\n",
    "        self.classification = ClassificationModel()\n",
    "        self.anchor_points = AnchorPoints(pyramid_levels=[3,], row=2, line=2)\n",
    "        self.fpn =  Decoder(256, 512, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features_fpn = self.fpn([features[1], features[2], features[3]])\n",
    "        \n",
    "        batch_size = features[0].shape[0]\n",
    "        regression_out = self.regression(features_fpn[1]) * 100 # 8x\n",
    "        classification_out = self.classification(features_fpn[1])\n",
    "        anchor_points_out = self.anchor_points(x).repeat(batch_size, 1, 1)\n",
    "        \n",
    "        output_coord = regression_out.to(device) + anchor_points_out.to(device)\n",
    "        \n",
    "        output_class = classification_out\n",
    "        out = {'pred_logits': output_class, 'pred_points': output_coord}\n",
    "       \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03cfb354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an instance of the model\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # device='cpu'\n",
    "# model = P2PNet()\n",
    "# PATH='D:/best_mae (1).pth'\n",
    "# # PATH=checkpoints_dir+'/latest.pth'\n",
    "\n",
    "# checkpoint = torch.load(PATH, map_location=device)\n",
    "# model.load_state_dict(checkpoint['model'])\n",
    "# model.to(device)\n",
    "# model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ffe4cd-af1b-4342-80ac-78e5a47ac00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device='cpu'\n",
    "model = P2PNet()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c513f56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "        return tensor\n",
    "\n",
    "def loading_data(data_root):\n",
    "    # the pre-proccssing transform\n",
    "    transform = standard_transforms.Compose([\n",
    "        standard_transforms.ToTensor(), \n",
    "        standard_transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    # create the training dataset\n",
    "    train_set = SHHA(data_root, train=True, transform=transform, patch=True, flip=True)\n",
    "    # create the validation dataset\n",
    "    val_set = SHHA(data_root, train=False, transform=transform)\n",
    "\n",
    "    return train_set, val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb67df73",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26d9f14b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import glob\n",
    "import scipy.io as io\n",
    "\n",
    "class SHHA(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for filename in os.listdir(root_dir):\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "                img_path = os.path.join(root_dir, filename)\n",
    "                self.images.append(img_path)\n",
    "                \n",
    "                label = os.path.join(root_dir, filename[:-4]+'.txt')\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert index <= len(self), 'index range error'\n",
    "\n",
    "        img_path = self.images[index]\n",
    "        gt_path = self.labels[index]\n",
    "        # load image and ground truth\n",
    "        img, point = load_data((img_path, gt_path))\n",
    "        # applu augumentation\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if True:\n",
    "            # data augmentation -> random scale\n",
    "            scale_range = [0.7, 1.3]\n",
    "            min_size = min(img.shape[1:])\n",
    "            scale = random.uniform(*scale_range)\n",
    "            # scale the image and points\n",
    "            if scale * min_size > 128:\n",
    "                img = torch.nn.functional.upsample_bilinear(img.unsqueeze(0), scale_factor=scale).squeeze(0)\n",
    "                point *= scale\n",
    "        # random crop augumentaiton\n",
    "        if True:\n",
    "            img, point = random_crop(img, point)\n",
    "            for i, _ in enumerate(point):\n",
    "                point[i] = torch.Tensor(point[i])\n",
    "        # random flipping\n",
    "        if True:\n",
    "            # random flip\n",
    "            img = torch.Tensor(img[:, :, :, ::-1].copy())\n",
    "            for i, _ in enumerate(point):\n",
    "                point[i][:, 0] = 128 - point[i][:, 0]\n",
    "\n",
    "        if not True:\n",
    "            point = [point]\n",
    "\n",
    "        img = torch.Tensor(img)\n",
    "        # pack up related infos\n",
    "        target = [{} for i in range(len(point))]\n",
    "        for i, _ in enumerate(point):\n",
    "            target[i]['point'] = torch.Tensor(point[i])\n",
    "            image_id = int(img_path.split('/')[-1].split('.')[0].split('_')[-1])\n",
    "            image_id = torch.Tensor([image_id]).long()\n",
    "            target[i]['image_id'] = image_id\n",
    "            target[i]['labels'] = torch.ones([point[i].shape[0]]).long()\n",
    "\n",
    "        return img, target\n",
    "\n",
    "\n",
    "def load_data(img_gt_path):\n",
    "    img_path, gt_path = img_gt_path\n",
    "    # load the images\n",
    "    img = cv2.imread(img_path)\n",
    "    img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    # load ground truth points\n",
    "    points = []\n",
    "    with open(gt_path) as f_label:\n",
    "        for line in f_label:\n",
    "            x = float(line.strip().split(' ')[0])\n",
    "            y = float(line.strip().split(' ')[1])\n",
    "            points.append([x, y])\n",
    "\n",
    "    return img, np.array(points)\n",
    "\n",
    "# random crop augumentation\n",
    "def random_crop(img, den, num_patch=2):\n",
    "    half_h = 128\n",
    "    half_w = 128\n",
    "    result_img = np.zeros([num_patch, img.shape[0], half_h, half_w])\n",
    "    result_den = []\n",
    "    # crop num_patch for each image\n",
    "    for i in range(num_patch):\n",
    "        start_h = random.randint(0, img.size(1) - half_h)\n",
    "        start_w = random.randint(0, img.size(2) - half_w)\n",
    "        end_h = start_h + half_h\n",
    "        end_w = start_w + half_w\n",
    "        # copy the cropped rect\n",
    "        result_img[i] = img[:, start_h:end_h, start_w:end_w]\n",
    "        # copy the cropped points\n",
    "        idx = (den[:, 0] >= start_w) & (den[:, 0] <= end_w) & (den[:, 1] >= start_h) & (den[:, 1] <= end_h)\n",
    "        # shift the corrdinates\n",
    "        record_den = den[idx]\n",
    "        record_den[:, 0] -= start_w\n",
    "        record_den[:, 1] -= start_h\n",
    "\n",
    "        result_den.append(record_den)\n",
    "\n",
    "    return result_img, result_den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38078023-a93b-495a-97b8-652838f85311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_crowd(batch):\n",
    "    # re-organize the batch\n",
    "    batch_new = []\n",
    "    for b in batch:\n",
    "        imgs, points = b\n",
    "        if imgs.ndim == 3:\n",
    "            imgs = imgs.unsqueeze(0)\n",
    "        for i in range(len(imgs)):\n",
    "            batch_new.append((imgs[i, :, :, :], points[i]))\n",
    "    batch = batch_new\n",
    "    batch = list(zip(*batch))\n",
    "    batch[0] = nested_tensor_from_tensor_list(batch[0])\n",
    "    return tuple(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "642edfba-3b63-4690-8ae4-0437591b60f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "# create the training dataset\n",
    "train_set = SHHA('D:/20230426CDI/data/train/', transform=transform)\n",
    "# create the validation dataset\n",
    "val_set = SHHA('D:/20230426CDI/data/test/',transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "509ca6a6-ba2a-4ae0-94c2-06a18de27bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=2\n",
    "data_loader_train = DataLoader(train_set, batch_size=batch_size, collate_fn=collate_fn_crowd,shuffle=True)\n",
    "data_loader_test = DataLoader(val_set, batch_size=batch_size, collate_fn=collate_fn_crowd,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a256fa-ebba-4a73-abdf-c78879aae1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3df512b-984d-4a3e-a27d-130b572f1075",
   "metadata": {},
   "source": [
    "# Show some annotation examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a12a14-43fc-4b43-b8c3-f383925629ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c890214a-d0a6-452e-beab-1784325b0bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e85c3ba6-9d61-48ee-b4b3-5ef27c31423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "from torch import Tensor\n",
    "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n",
    "    # TODO make this more general\n",
    "    if tensor_list[0].ndim == 3:\n",
    "\n",
    "        # TODO make it support different-sized images\n",
    "        max_size = _max_by_axis_pad([list(img.shape) for img in tensor_list])\n",
    "        # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))\n",
    "        batch_shape = [len(tensor_list)] + max_size\n",
    "        b, c, h, w = batch_shape\n",
    "        dtype = tensor_list[0].dtype\n",
    "        device = tensor_list[0].device\n",
    "        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
    "        for img, pad_img in zip(tensor_list, tensor):\n",
    "            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
    "    else:\n",
    "        raise ValueError('not supported')\n",
    "    return tensor\n",
    "\n",
    "def _max_by_axis_pad(the_list):\n",
    "    # type: (List[List[int]]) -> List[int]\n",
    "    maxes = the_list[0]\n",
    "    for sublist in the_list[1:]:\n",
    "        for index, item in enumerate(sublist):\n",
    "            maxes[index] = max(maxes[index], item)\n",
    "\n",
    "    block = 128\n",
    "\n",
    "    for i in range(2):\n",
    "        maxes[i+1] = ((maxes[i+1] - 1) // block + 1) * block\n",
    "    return maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5196f73e-e23a-46f0-ba17-96448f2eef37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\piyalong\\AppData\\Local\\anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\nn\\functional.py:4079: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    }
   ],
   "source": [
    "unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "\n",
    "for batch_images, batch_labels in data_loader_test:\n",
    "    for image,target in zip(batch_images, batch_labels):\n",
    "        image=unorm(image)\n",
    "        landmarks=target['point']\n",
    "        image_numpy=image.cpu().numpy().copy()\n",
    "        image_numpy = np.transpose(image_numpy, (1, 2, 0))\n",
    "        if len(landmarks)>5:\n",
    "            print(image_numpy.shape,image_numpy.max())\n",
    "            plt.imshow(image_numpy)\n",
    "            plt.scatter(landmarks[:, 0], landmarks[:, 1], s=20, marker='o', c='y')\n",
    "            break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce107e3-62cb-4230-b874-54c9ddd0fd9a",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eddccc3-2499-4e8e-a239-f73e366c8a86",
   "metadata": {},
   "source": [
    "## Hungarian match again to see TP. This matching is different from the training match because training matches every possible point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c4ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580d05e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769135bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b88cf3f-8ffb-49a6-88f7-5613dd5d692e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3339fad-4c53-4a84-af96-2c97b64519cc",
   "metadata": {},
   "source": [
    "# Old loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09588689-95b4-4ecc-85f6-c54663324c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetCriterion_Crowd(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, matcher,losses = ['labels', 'points']):\n",
    "        \"\"\" Create the criterion.\n",
    "        Parameters:\n",
    "            num_classes: number of object categories, omitting the special no-object category\n",
    "            matcher: module able to compute a matching between targets and proposals\n",
    "            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n",
    "            eos_coef: relative classification weight applied to the no-object category\n",
    "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.matcher = HungarianMatcher_Crowd()\n",
    "        self.weight_dict = {'loss_ce': 1, 'loss_points': 0.5}\n",
    "        # self.eos_coef = eos_coef\n",
    "        self.losses = losses\n",
    "        empty_weight = torch.ones(self.num_classes + 1)\n",
    "        empty_weight[0] = 1\n",
    "        self.register_buffer('empty_weight', empty_weight)\n",
    "\n",
    "    def loss_labels(self, outputs, targets, indices, num_points):\n",
    "        \"\"\"Classification loss (NLL)\n",
    "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n",
    "        \"\"\"\n",
    "        assert 'pred_logits' in outputs\n",
    "        src_logits = outputs['pred_logits']\n",
    "        # print(indices)\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)]).to(device)\n",
    "        target_classes = torch.full(src_logits.shape[:2], 0,\n",
    "                                    dtype=torch.int64, device=device)\n",
    "        target_classes[idx] = target_classes_o\n",
    "        # print(src_logits.transpose(1, 2).device,target_classes.device)\n",
    "        loss_ce = F.cross_entropy(src_logits.transpose(1, 2).to(device), target_classes, self.empty_weight.to(device))\n",
    "        \n",
    "\n",
    "        losses = {'loss_ce': loss_ce}\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def loss_points(self, outputs, targets, indices, num_points):\n",
    "\n",
    "        assert 'pred_points' in outputs\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        src_points = outputs['pred_points'][idx]\n",
    "        target_points = torch.cat([t['point'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
    "\n",
    "        loss_bbox = F.mse_loss(src_points.to(device), target_points.to(device), reduction='none')\n",
    "\n",
    "        losses = {}\n",
    "        losses['loss_point'] = loss_bbox.sum() / num_points\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "        # permute predictions following indices\n",
    "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
    "        src_idx = torch.cat([src for (src, _) in indices])\n",
    "        return batch_idx, src_idx\n",
    "\n",
    "    def _get_tgt_permutation_idx(self, indices):\n",
    "        # permute targets following indices\n",
    "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
    "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
    "        return batch_idx, tgt_idx\n",
    "\n",
    "    def get_loss(self, loss, outputs, targets, indices, num_points, **kwargs):\n",
    "        loss_map = {\n",
    "            'labels': self.loss_labels,\n",
    "            'points': self.loss_points,\n",
    "        }\n",
    "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
    "        return loss_map[loss](outputs, targets, indices, num_points, **kwargs)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\" This performs the loss computation.\n",
    "        Parameters:\n",
    "             outputs: dict of tensors, see the output specification of the model for the format\n",
    "             targets: list of dicts, such that len(targets) == batch_size.\n",
    "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
    "        \"\"\"\n",
    "        output1 = {'pred_logits': outputs['pred_logits'], 'pred_points': outputs['pred_points']}\n",
    "\n",
    "        indices1 = self.matcher(output1, targets)\n",
    "\n",
    "        num_points = sum(len(t[\"labels\"]) for t in targets)\n",
    "        num_points = torch.as_tensor([num_points], dtype=torch.float, device=next(iter(output1.values())).device)\n",
    "        if is_dist_avail_and_initialized():\n",
    "            torch.distributed.all_reduce(num_points)\n",
    "        num_boxes = torch.clamp(num_points / get_world_size(), min=1).item()\n",
    "\n",
    "        losses = {}\n",
    "        for loss in self.losses:\n",
    "            losses.update(self.get_loss(loss, output1, targets, indices1, num_boxes))\n",
    "\n",
    "        return losses\n",
    "        \n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bee1f28-23ee-4d69-950b-12ef20c4c1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torch import nn\n",
    "import torch.distributed as dist\n",
    "\n",
    "class HungarianMatcher_Crowd(nn.Module):\n",
    "    \"\"\"This class computes an assignment between the targets and the predictions of the network\n",
    "\n",
    "    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n",
    "    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n",
    "    while the others are un-matched (and thus treated as non-objects).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cost_class: float = 1, cost_point: float = 1):\n",
    "        \"\"\"Creates the matcher\n",
    "\n",
    "        Params:\n",
    "            cost_class: This is the relative weight of the foreground object\n",
    "            cost_point: This is the relative weight of the L1 error of the points coordinates in the matching cost\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cost_class = cost_class\n",
    "        self.cost_point = cost_point\n",
    "        assert cost_class != 0 or cost_point != 0, \"all costs cant be 0\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\" Performs the matching\n",
    "\n",
    "        Params:\n",
    "            outputs: This is a dict that contains at least these entries:\n",
    "                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n",
    "                 \"points\": Tensor of dim [batch_size, num_queries, 2] with the predicted point coordinates\n",
    "\n",
    "            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n",
    "                 \"labels\": Tensor of dim [num_target_points] (where num_target_points is the number of ground-truth\n",
    "                           objects in the target) containing the class labels\n",
    "                 \"points\": Tensor of dim [num_target_points, 2] containing the target point coordinates\n",
    "\n",
    "        Returns:\n",
    "            A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
    "                - index_i is the indices of the selected predictions (in order)\n",
    "                - index_j is the indices of the corresponding selected targets (in order)\n",
    "            For each batch element, it holds:\n",
    "                len(index_i) = len(index_j) = min(num_queries, num_target_points)\n",
    "        \"\"\"\n",
    "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
    "\n",
    "        # We flatten to compute the cost matrices in a batch\n",
    "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
    "        out_points = outputs[\"pred_points\"].flatten(0, 1).cpu()  # [batch_size * num_queries, 2]\n",
    "\n",
    "        # Also concat the target labels and points\n",
    "        # tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
    "        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
    "        tgt_points = torch.cat([v[\"point\"] for v in targets]).cpu()\n",
    "\n",
    "        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
    "        # but approximate it in 1 - proba[target class].\n",
    "        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
    "        cost_class = -out_prob[:, tgt_ids].cpu()\n",
    "        # print(tgt_ids.shape)\n",
    "        # Compute the L2 cost between point\n",
    "        cost_point = torch.cdist(out_points, tgt_points, p=2).cpu()\n",
    "\n",
    "        # Compute the giou cost between point\n",
    "\n",
    "        # Final cost matrix\n",
    "        C = self.cost_point * cost_point + self.cost_class * cost_class\n",
    "        C = C.view(bs, num_queries, -1).cpu()\n",
    "        sizes = [len(v[\"point\"]) for v in targets]\n",
    "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
    "\n",
    "        # print(C.shape)\n",
    "\n",
    "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beaf2031-b8a2-4e9b-b06a-a2556eab4baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher=HungarianMatcher_Crowd()\n",
    "\n",
    "criterion = SetCriterion_Crowd(1,  matcher=matcher)\n",
    "# idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "499e07e6-ac00-4d16-b4cf-d2dcd454dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_images, batch_labels in data_loader_train:\n",
    "    outputs = model(batch_images.to(device))\n",
    "    criterion(outputs,batch_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91dc7a0-53d3-4f2c-822d-bb27cca767ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff45612b-b58b-48f8-8478-9852d74acafe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8c830df-454a-4e9b-9d7b-e44ebbb54bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1024, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['pred_logits'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e6342c7-459a-4579-8481-3447a87239ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_ce': tensor(0.6825, grad_fn=<NllLoss2DBackward0>),\n",
       " 'loss_point': tensor(1.4438, grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx=matcher(outputs,batch_labels)\n",
    "criterion(outputs,batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a7cf4e-2635-4a7f-9a07-3a78a3efc67c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b3d39-7ca3-461c-8018-47686610d673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "099f9437",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31d7f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dict(input_dict, average=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_dict (dict): all the values will be reduced\n",
    "        average (bool): whether to do average or sum\n",
    "    Reduce the values in the dictionary from all processes so that all processes\n",
    "    have the averaged results. Returns a dict with the same fields as\n",
    "    input_dict, after reduction.\n",
    "    \"\"\"\n",
    "    world_size = get_world_size()\n",
    "    if world_size < 2:\n",
    "        return input_dict\n",
    "    with torch.no_grad():\n",
    "        names = []\n",
    "        values = []\n",
    "        # sort the keys so that they are consistent across processes\n",
    "        for k in sorted(input_dict.keys()):\n",
    "            names.append(k)\n",
    "            values.append(input_dict[k])\n",
    "        values = torch.stack(values, dim=0)\n",
    "        dist.all_reduce(values)\n",
    "        if average:\n",
    "            values /= world_size\n",
    "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
    "    return reduced_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ffc009c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train_loss: 0.016503769904375076\n",
      "Epoch 0, Val_loss: 0.00021499402646441013\n",
      "Epoch 1, Train_loss: 0.0015319137601181865\n",
      "Epoch 1, Val_loss: 3.858795025735162e-05\n",
      "Epoch 2, Train_loss: 8.894098755263258e-06\n",
      "Epoch 2, Val_loss: 0.008533523418009281\n",
      "Epoch 3, Train_loss: 0.010529572144150734\n",
      "Epoch 3, Val_loss: 0.0038082667160779238\n",
      "Epoch 4, Train_loss: 6.880614091642201e-05\n",
      "Epoch 4, Val_loss: 0.025980614125728607\n",
      "Epoch 5, Train_loss: 0.008365953341126442\n",
      "Epoch 5, Val_loss: 0.004917907994240522\n",
      "Epoch 6, Train_loss: 0.003158367471769452\n",
      "Epoch 6, Val_loss: 0.003961137495934963\n",
      "Epoch 7, Train_loss: 0.020851483568549156\n",
      "Epoch 7, Val_loss: 1.714516201900551e-06\n",
      "Epoch 8, Train_loss: 0.005635589361190796\n",
      "Epoch 8, Val_loss: 1.107107323150558e-06\n",
      "Epoch 9, Train_loss: 0.0030270537827163935\n",
      "Epoch 9, Val_loss: 0.002179906703531742\n",
      "Epoch 10, Train_loss: 1.8501614249544218e-05\n",
      "Epoch 10, Val_loss: 2.3301588498725323e-06\n",
      "Epoch 11, Train_loss: 0.010821078903973103\n",
      "Epoch 11, Val_loss: 5.598885309154866e-06\n",
      "Epoch 12, Train_loss: 0.0040242839604616165\n",
      "Epoch 12, Val_loss: 0.005656383465975523\n",
      "Epoch 13, Train_loss: 8.951699783210643e-06\n",
      "Epoch 13, Val_loss: 1.5634042938472703e-05\n",
      "Epoch 14, Train_loss: 1.3043375474808272e-05\n",
      "Epoch 14, Val_loss: 9.132091918218066e-07\n",
      "Epoch 15, Train_loss: 0.0005959782283753157\n",
      "Epoch 15, Val_loss: 8.473793968732934e-06\n",
      "Epoch 16, Train_loss: 0.0027041484136134386\n",
      "Epoch 16, Val_loss: 8.174446293196524e-07\n",
      "Epoch 17, Train_loss: 0.0010923686204478145\n",
      "Epoch 17, Val_loss: 0.01219312846660614\n",
      "Epoch 18, Train_loss: 0.00022841246391180903\n",
      "Epoch 18, Val_loss: 1.282891844311962e-06\n",
      "Epoch 19, Train_loss: 6.120782472862629e-06\n",
      "Epoch 19, Val_loss: 7.509161514462903e-05\n",
      "Epoch 20, Train_loss: 0.0035550326574593782\n",
      "Epoch 20, Val_loss: 0.0004485809477046132\n",
      "Epoch 21, Train_loss: 0.0028940956108272076\n",
      "Epoch 21, Val_loss: 0.006480652838945389\n",
      "Epoch 22, Train_loss: 1.4008681318955496e-05\n",
      "Epoch 22, Val_loss: 0.011277970857918262\n",
      "Epoch 23, Train_loss: 0.0020420635119080544\n",
      "Epoch 23, Val_loss: 0.0037231945898383856\n",
      "Epoch 24, Train_loss: 9.849150956142694e-05\n",
      "Epoch 24, Val_loss: 0.011230004020035267\n",
      "Epoch 25, Train_loss: 0.00010320561705157161\n",
      "Epoch 25, Val_loss: 1.531729196813103e-07\n",
      "Epoch 26, Train_loss: 0.003630345221608877\n",
      "Epoch 26, Val_loss: 0.0009212855366058648\n",
      "Epoch 27, Train_loss: 0.005135753657668829\n",
      "Epoch 27, Val_loss: 0.0020814614836126566\n",
      "Epoch 28, Train_loss: 9.142846465692855e-06\n",
      "Epoch 28, Val_loss: 0.005611540284007788\n",
      "Epoch 29, Train_loss: 0.0004868753894697875\n",
      "Epoch 29, Val_loss: 0.00020584807498380542\n",
      "Epoch 30, Train_loss: 0.006216494366526604\n",
      "Epoch 30, Val_loss: 5.568696906266268e-06\n",
      "Epoch 31, Train_loss: 1.8602021327751572e-06\n",
      "Epoch 31, Val_loss: 4.1573801468075544e-07\n",
      "Epoch 32, Train_loss: 5.660754140990321e-06\n",
      "Epoch 32, Val_loss: 0.0035428665578365326\n",
      "Epoch 33, Train_loss: 1.8272830857313238e-05\n",
      "Epoch 33, Val_loss: 0.001145219779573381\n",
      "Epoch 34, Train_loss: 2.544880953792017e-05\n",
      "Epoch 34, Val_loss: 8.544843410618341e-08\n",
      "Epoch 35, Train_loss: 2.4743947506067343e-05\n",
      "Epoch 35, Val_loss: 0.00023766898084431887\n",
      "Epoch 36, Train_loss: 0.00034715660149231553\n",
      "Epoch 36, Val_loss: 0.0001348644436802715\n",
      "Epoch 37, Train_loss: 0.0004591378674376756\n",
      "Epoch 37, Val_loss: 0.007475770078599453\n",
      "Epoch 38, Train_loss: 0.0004030343552585691\n",
      "Epoch 38, Val_loss: 1.600667201273609e-05\n",
      "Epoch 39, Train_loss: 0.007535272277891636\n",
      "Epoch 39, Val_loss: 0.00027298732311464846\n",
      "Epoch 40, Train_loss: 0.0007117813802324235\n",
      "Epoch 40, Val_loss: 1.6065260410869087e-07\n",
      "Epoch 41, Train_loss: 0.000530349847394973\n",
      "Epoch 41, Val_loss: 5.609837899100967e-06\n",
      "Epoch 42, Train_loss: 8.797774171398487e-06\n",
      "Epoch 42, Val_loss: 3.354940781719051e-05\n",
      "Epoch 43, Train_loss: 0.0001941997034009546\n",
      "Epoch 43, Val_loss: 0.0075096068903803825\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     17\u001b[0m criterion\u001b[38;5;241m.\u001b[39mtrain()  \n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m samples, targets \u001b[38;5;129;01min\u001b[39;00m data_loader_train:\n\u001b[0;32m     19\u001b[0m     samples \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     20\u001b[0m     targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\jupyterlab\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[10], line 35\u001b[0m, in \u001b[0;36mSHHA.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     33\u001b[0m gt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[index]\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# load image and ground truth\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m img, point \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# applu augumentation\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[10], line 84\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(img_gt_path)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# load ground truth points\u001b[39;00m\n\u001b[0;32m     83\u001b[0m points \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgt_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f_label:\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f_label:\n\u001b[0;32m     86\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\jupyterlab\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\jupyterlab\\lib\\_bootlocale.py:11\u001b[0m, in \u001b[0;36mgetpreferredencoding\u001b[1;34m(do_setlocale)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01m_locale\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetpreferredencoding\u001b[39m(do_setlocale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mutf8_mode:\n\u001b[0;32m     13\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTF-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "matcher=HungarianMatcher_Crowd()\n",
    "criterion = SetCriterion_Crowd(1,  matcher=matcher)\n",
    "\n",
    "train_writer = SummaryWriter(tensorboard_dir+'train/')\n",
    "test_writer = SummaryWriter(tensorboard_dir+'test/')\n",
    "\n",
    "loss_history=[]\n",
    "train_step=test_step=0\n",
    "\n",
    "# Adam is used by default\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001 )\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',verbose =True)\n",
    "# iterate all training samples\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    criterion.train()  \n",
    "    for samples, targets in data_loader_train:\n",
    "        samples = samples.to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        # forward\n",
    "        outputs = model(samples)\n",
    "        criterion(outputs,targets)\n",
    "        # calc the losses\n",
    "        loss_dict = criterion(outputs, targets)\n",
    "        weight_dict = criterion.weight_dict\n",
    "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "        # reduce all losses\n",
    "        loss_dict_reduced = reduce_dict(loss_dict)\n",
    "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
    "                                      for k, v in loss_dict_reduced.items()}\n",
    "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
    "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
    "        losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
    "\n",
    "        loss_value = losses_reduced_scaled.item()\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        if 0.1 > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "\n",
    "    if train_writer is not None:\n",
    "        train_writer.add_scalar('train/loss', losses, epoch)\n",
    "\n",
    "    checkpoint_latest_path = os.path.join(checkpoints_dir, 'latest.pth')\n",
    "    torch.save({ 'model': model.state_dict(),}, checkpoint_latest_path)\n",
    "\n",
    "    print(\"Epoch {}, Train_loss: {}\".format(epoch,loss_value))\n",
    "\n",
    "    #Evaluation\n",
    "    torch.no_grad()\n",
    "    model.eval()\n",
    "    for samples, targets in data_loader_train:\n",
    "        samples = samples.to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        # forward\n",
    "        outputs = model(samples)\n",
    "        criterion(outputs,targets)\n",
    "        # calc the losses\n",
    "        loss_dict = criterion(outputs, targets)\n",
    "        weight_dict = criterion.weight_dict\n",
    "        loss_eval = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "        loss_history.append(loss_eval.detach().numpy()) \n",
    "        test_step+=1\n",
    "    if test_writer is not None:\n",
    "        test_writer.add_scalar('test/loss_eval',loss_eval, epoch)\n",
    "    print(\"Epoch {}, Val_loss: {}\".format(epoch,loss_eval))\n",
    "    \n",
    "    if abs(np.min(loss_history) - loss_eval) < 0.01:\n",
    "        checkpoint_best_path = os.path.join(checkpoints_dir, 'best_mae.pth')\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "        }, checkpoint_best_path)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae748f1-eca5-4a9e-992a-936e9c6ea799",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d6330f-ed3f-484e-99e5-5e10fa4f494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e8128f-5459-40c9-b76b-317f396d49cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a98737c-ea1a-4c56-b7e3-fb3a9a456147",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_eval=loss_eval.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9071b277-f0b7-43d1-8558-413dd2742d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e62fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b02eeee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83086bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bce949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(p1, p2):\n",
    "    return np.sqrt(np.sum((p1 - p2) ** 2))\n",
    "\n",
    "def match_resutls(predicted_points, gt_points):\n",
    "\n",
    "    if len(predicted_points) >= len(gt_points):\n",
    "        n_rows =n_cols=len(predicted_points)\n",
    "    else:\n",
    "        n_cols = n_rows =len(gt_points)\n",
    "\n",
    "    cost_matrix = np.zeros((n_rows, n_cols))\n",
    "\n",
    "    for i, pred_point in enumerate(predicted_points):\n",
    "        for j, gt_point in enumerate(gt_points):\n",
    "            cost_matrix[i, j] = euclidean_distance(pred_point, gt_point)\n",
    "            \n",
    "    row_indices, col_indices = linear_sum_assignment(cost_matrix)\n",
    "    \n",
    "    return cost_matrix, row_indices, col_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d2eed-2912-4007-a6e0-ed6b33a1a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tp=total_gt=total_pred=0\n",
    "i=0\n",
    "\n",
    "for batch_images, batch_labels in data_loader_test:\n",
    "    with torch.no_grad():\n",
    "        outputs =     model(batch_images.to(device))\n",
    "\n",
    "    for image,target,pred_logits, pred_points in zip(batch_images, batch_labels,outputs['pred_logits'],outputs['pred_points']):\n",
    "        image=unorm(image)\n",
    "        outputs_scores = torch.nn.functional.softmax(pred_logits, -1)[:, 1]\n",
    "        points = pred_points[outputs_scores > 0.0001].cpu().numpy()\n",
    "        \n",
    "        image_numpy = np.transpose(image.cpu(), (1, 2, 0))\n",
    "        img_to_draw=image_numpy.numpy().copy() \n",
    "                \n",
    "        y_true=target['point'].detach().cpu().numpy()\n",
    "        \n",
    "        cost_matrix,row_indices, col_indices=match_resutls(points, y_true)\n",
    "        size=50\n",
    "        tp=0\n",
    "\n",
    "        yellow2=plt.scatter(y_true[:, 0], y_true[:, 1], s=size, marker='o', c='y', label='GT')\n",
    "        yellow1=plt.scatter(points[:, 0], points[:, 1], s=size, marker='+', c='y', label='CV')\n",
    "        \n",
    "        for row, col in zip(row_indices, col_indices):\n",
    "            if row < len(points) and col < len(y_true):\n",
    "                pred_point = points[row]\n",
    "                gt_point = y_true[col]\n",
    "                if cost_matrix[row, col]<size:\n",
    "                    circles=plt.scatter(gt_point[0], gt_point[1], s=size, marker='o', c='g', label='GT')\n",
    "                    crosses=plt.scatter(pred_point[0], pred_point[1], s=size, marker='+', c='w', label='CV')\n",
    "                    tp+=1\n",
    "        \n",
    "        total_tp+=tp\n",
    "        total_gt+=len(y_true)\n",
    "        total_pred+=len(points)\n",
    "        if total_tp>0:\n",
    "            print(total_tp)\n",
    "            print(f' TP {tp} precision {tp/len(points) } recall {tp/len(y_true)}')\n",
    "            # print(target['image_ids'].numpy())\n",
    "            # custom_legend_labels = ['TP CV', 'TP GT', 'Unmatched CV','Unmatched GT']\n",
    "            # custom_legend_handles = [crosses, circles,yellow1,yellow2]\n",
    "            # # Add a custom legend\n",
    "            # plt.legend(custom_legend_handles, custom_legend_labels)\n",
    "            plt.imshow(img_to_draw)\n",
    "            # plt.savefig(f'Evaluation/{i}.jpg',bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "        i+=1\n",
    "        # break\n",
    "    # break\n",
    "# print(f'total TP {total_tp} precision {total_tp/total_pred } recall {total_tp/total_gt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44c2c4-4837-4841-9f24-1bdbd57741ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_scores.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa27a8-31c8-485b-9723-b20fed9d0ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1b661d8c8d3dcdc0a04567d19d202ef4d670cb5996a93bcce7891944b0cd9be3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
